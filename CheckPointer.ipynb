Here’s the corrected, well‑formed JSON for the notebook. You can paste it into a file named banking_chatbot_demos.ipynb.
jsonDownloadCopy code{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Banking-focused LangGraph/LangChain demos\n",
        "\n",
        "The notebooks below translate the examples from the **langgraph/7_chatbot** repository into banking-domain scenarios while preserving the original concepts (basic chat, memory, tool use, retrieval, structured output, streaming, and multi-agent routing). All code works with **LangChain-1.x** and **LangGraph-1.x** and uses Groq’s **llama-3.1-8b-instant** model.\n",
        "\n",
        "Each demo ends with a short **practice assignment**. The next notebook (`banking_chatbot_solutions.ipynb`) contains the reference solutions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "imports"
        ]
      },
      "outputs": [],
      "source": [
        "# Imports – common to all demos\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import InMemoryCheckpointSaver\n",
        "from langgraph.prebuilt import ToolNode, tools\n",
        "\n",
        "# Groq LLM wrapper (compatible with LangChain 1.x)\n",
        "def get_llm():\n",
        "    return ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo-1 – Simple banking chatbot\n",
        "\n",
        "Core concept: **single-turn LLM call**.\n",
        "\n",
        "- **Scenario**: A customer asks for the current balance of a checking account.\n",
        "- **Prompt**: Provide a concise answer without disclosing internal IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "demo1"
        ]
      },
      "outputs": [],
      "source": [
        "def simple_balance_chat(user_input: str) -> str:\n",
        "    llm = get_llm()\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful banking assistant. Answer only the question asked. Do not ask for additional information.\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    return chain.invoke({\"question\": user_input}).content\n",
        "\n",
        "# Example run\n",
        "print(simple_balance_chat(\"What is my checking account balance?\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Practice Assignment-1**\n",
        "\n",
        "Modify `simple_balance_chat` so that it can handle two types of queries:\n",
        "1. Balance check (as before).\n",
        "2. Recent transaction list (e.g., “Show my last three credit-card transactions”).\n",
        "\n",
        "The function should detect the intent from the user input and respond appropriately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo-2 – Stateful chatbot with memory (conversation history)\n",
        "\n",
        "Core concept: **memory node** in LangGraph.\n",
        "\n",
        "- **Scenario**: A customer asks follow-up questions about a loan product.\n",
        "- The graph stores the previous messages so the LLM can refer back.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "demo2"
        ]
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List\n",
        "\n",
        "class ChatState(TypedDict):\n",
        "    messages: List[HumanMessage | AIMessage]\n",
        "\n",
        "def build_chatbot_graph():\n",
        "    llm = get_llm()\n",
        "\n",
        "    def call_llm(state: ChatState):\n",
        "        response = llm.invoke(state[\"messages\"])\n",
        "        return {\"messages\": state[\"messages\"] + [response]}\n",
        "\n",
        "    workflow = StateGraph(ChatState)\n",
        "    workflow.add_node(\"llm\", call_llm)\n",
        "    workflow.set_entry_point(\"llm\")\n",
        "    workflow.add_edge(\"llm\", END)\n",
        "    return workflow.compile(checkpointer=InMemoryCheckpointSaver())\n",
        "\n",
        "# Run a short conversation\n",
        "graph = build_chatbot_graph()\n",
        "init_state = {\"messages\": [HumanMessage(content=\"Tell me about our personal loan rates.\")]}\n",
        "result = graph.invoke(init_state)\n",
        "print(result[\"messages\"][-1].content)\n",
        "\n",
        "# Follow-up\n",
        "follow_up = {\"messages\": result[\"messages\"] + [HumanMessage(content=\"What is the minimum credit score required?\")]}\n",
        "result2 = graph.invoke(follow_up)\n",
        "print(result2[\"messages\"][-1].content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Practice Assignment-2**\n",
        "\n",
        "Add a **summary memory** node that keeps a short bullet-point summary of the conversation (max-3-lines). The LLM should be prompted with both the full message list **and** the summary when generating a response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo-3 – Tool use (account-balance lookup)\n",
        "\n",
        "Core concept: **function-calling / tool node**.\n",
        "\n",
        "- The LLM decides whether to call the `get_balance` tool.\n",
        "- The tool simulates a secure lookup (hard-coded dictionary for demo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "demo3"
        ]
      },
      "outputs": [],
      "source": [
        "# Simple in-memory account store (DO NOT use in production)\n",
        "ACCOUNT_DATA = {\n",
        "    \"checking\": 3452.78,\n",
        "    \"savings\": 12301.50,\n",
        "    \"credit\": -1205.33,\n",
        "}\n",
        "\n",
        "def get_balance(account_type: str) -> str:\n",
        "    bal = ACCOUNT_DATA.get(account_type.lower())\n",
        "    if bal is None:\n",
        "        return f\"I couldn't find a {account_type} account.\"\n",
        "    return f\"Your {account_type} balance is ${bal:,.2f}.\"\n",
        "\n",
        "# Register the tool with LangGraph\n",
        "balance_tool = tools.ToolSpec.from_function(\n",
        "    func=get_balance,\n",
        "    name=\"get_balance\",\n",
        "    description=\"Retrieve the balance for a given account type (checking, savings, credit).\",\n",
        "    args_schema={\"account_type\": str},\n",
        ")\n",
        "\n",
        "def build_tool_graph():\n",
        "    llm = get_llm()\n",
        "\n",
        "    def router(state: ChatState):\n",
        "        last_message = state[\"messages\"][-1].content.lower()\n",
        "        if \"balance\" in last_message:\n",
        "            return {\"route\": \"tool\"}\n",
        "        return {\"route\": \"llm\"}\n",
        "\n",
        "    def llm_node(state: ChatState):\n",
        "        response = llm.invoke(state[\"messages\"])\n",
        "        return {\"messages\": state[\"messages\"] + [response]}\n",
        "\n",
        "    workflow = StateGraph(ChatState)\n",
        "    workflow.add_node(\"router\", router)\n",
        "    workflow.add_node(\"llm\", llm_node)\n",
        "    workflow.add_node(\"tool\", ToolNode([balance_tool]))\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"router\",\n",
        "        lambda state: state[\"route\"],\n",
        "        {\n",
        "            \"llm\": \"llm\",\n",
        "            \"tool\": \"tool\"\n",
        "        }\n",
        "    )\n",
        "    workflow.add_edge(\"llm\", END)\n",
        "    workflow.add_edge(\"tool\", END)\n",
        "    workflow.set_entry_point(\"router\")\n",
        "    return workflow.compile(checkpointer=InMemoryCheckpointSaver())\n",
        "\n",
        "tool_graph = build_tool_graph()\n",
        "state = {\"messages\": [HumanMessage(content=\"What is my savings balance?\")]}\n",
        "out = tool_graph.invoke(state)\n",
        "print(out[\"messages\"][-1].content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Practice Assignment-3**\n",
        "\n",
        "Add a second tool `transfer_funds(source: str, destination: str, amount: float)` that simulates an intra-bank transfer. Update the graph so the LLM can decide between the two tools based on the user request.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo-4 – Retrieval-augmented generation (FAQ lookup)\n",
        "\n",
        "Core concept: **vector store + RAG**.\n",
        "\n",
        "- We embed a small set of banking FAQs.\n",
        "- The LLM retrieves the most relevant snippet before answering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "demo4"
        ]
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import FakeEmbeddings\n",
        "\n",
        "# Small FAQ corpus\n",
        "FAQS = [\n",
        "    \"How can I reset my online banking password?\",\n",
        "    \"What is the interest rate for our 1-year fixed deposit?\",\n",
        "    \"What are the fees for an international wire transfer?\",\n",
        "    \"How do I enable card-to-card payments?\",\n",
        "]\n",
        "\n",
        "emb = FakeEmbeddings(size=768)  # placeholder; replace with a real embedding model for production\n",
        "vectorstore = FAISS.from_texts(FAQS, embedding=emb)\n",
        "\n",
        "def rag_answer(question: str) -> str:\n",
        "    llm = get_llm()\n",
        "    docs = vectorstore.similarity_search(question, k=2)\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a banking support bot. Use the provided context to answer the user's question concisely.\"),\n",
        "        (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    return chain.invoke({\"context\": context, \"question\": question}).content\n",
        "\n",
        "print(rag_answer(\"What are the fees for sending money abroad?\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Practice Assignment-4**\n",
        "\n",
        "Replace `FakeEmbeddings` with a real embedding model (e.g., `OpenAIEmbeddings`). Add a **metadata filter** that only returns FAQs tagged for the \"international\" category when the user mentions “foreign” or “abroad”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo-5 – Structured output (loan eligibility JSON)\n",
        "\n",
        "Core concept: **pydantic schema** + `output_parser`.\n",
        "\n",
        "- The LLM returns a JSON object indicating eligibility, approved amount, and interest rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "demo5"
        ]
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "class LoanResult(BaseModel):\n",
        "    eligible: bool = Field(description=\"Whether the applicant qualifies for the loan\")\n",
        "    approved_amount: float = Field(description=\"Maximum loan amount in USD\")\n",
        "    interest_rate: float = Field(description=\"Annual interest rate as a percentage\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=LoanResult)\n",
        "\n",
        "def loan_eligibility(prompt: str) -> LoanResult:\n",
        "    llm = get_llm()\n",
        "    template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a loan officer. Respond with a JSON object that matches the given schema.\"),\n",
        "        (\"human\", \"{question}\\n\\n{format_instructions}\")\n",
        "    ])\n",
        "    chain = template | llm | parser\n",
        "    return chain.invoke({\n",
        "        \"question\": prompt,\n",
        "        \"format_instructions\": parser.get_format_instructions()\n",
        "    })\n",
        "\n",
        "example = loan_eligibility(\"I have a credit score of 720, annual income $85k, and no existing debts.\")\n",
        "print(example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Practice Assignment-5**\n",
        "\n",
        "Create a new schema `AccountOpeningResult` with fields `approved: bool`, `account_number: str | None`, and `reason: str`. Write a prompt that asks the LLM to decide if a user can open a new checking account based on age and citizenship.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo-6 – Streaming responses (real-time chat UI)\n",
        "\n",
        "Core concept: **token-wise streaming**.\n",
        "\n",
        "- We use Groq’s streaming API to print tokens as they arrive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "demo6"
        ]
      },
      "outputs": [],
      "source": [
        "def stream_chat(question: str):\n",
        "    llm = get_llm()\n",
        "    # `stream=True` enables token streaming in LangChain 1.x\n",
        "    for chunk in llm.stream([HumanMessage(content=question)]):\n",
        "        print(chunk.content, end=\"\", flush=True)\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}